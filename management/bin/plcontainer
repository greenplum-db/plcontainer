#!/usr/bin/env python

#------------------------------------------------------------------------------
#
# 
# Copyright (c) 2016-Present Pivotal Software, Inc
#
#------------------------------------------------------------------------------

import os
import sys
import hashlib
import tempfile
import traceback
import datetime
import xml.dom.minidom
import commands
import argparse
from xml.etree import ElementTree

try:
    from gppylib import gplog
    from gppylib.commands.base import WorkerPool, REMOTE, Command
    from gppylib.commands.unix import Scp, FileDirExists, getLocalHostname, getUserName
    from gppylib.db import dbconn
    from gppylib import gparray
    from gppylib import userinput
except ImportError, e:
    sys.exit('ERROR: Cannot import Greenplum modules.  Please check that you have sourced greenplum_path.sh.  Detail: ' + str(e))

PLCONTAINER_CONFIG_FILE = 'plcontainer_configuration.xml'
PLCONTAINER_GPDB_DIR = 'share/postgresql/plcontainer'
PLCONTAINER_PYCLIENT_FILE = "pyclient"
PLCONTAINER_PYCLIENT_DIR = "bin/pyclient"
PL_BATCH_SIZE = 3
TMP = '/tmp'
TAR_GZ = '.tar.gz'

global GPHOME

def parseargs():
    parser = argparse.ArgumentParser()

    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")
    subparsers = parser.add_subparsers(dest='subparser_name')

    config_parser = subparsers.add_parser('configure', help="Configure container image name and " +
                        "location as well as shared folder, need to combine with -n and -i, usage: -n IMAGENAME -i IMAGEURL")
    install_parser = subparsers.add_parser('install', help='Install a dokcer image and add it to configuration file')

    # for configure
    config_parser.add_argument("-n", "--name", dest="name", help="Configure container image name")
    config_parser.add_argument("-i", "--image", dest="image", help="Configure container image location, support Tar.gz image" +
                        " file or docker image url")
    config_parser.add_argument("-v", "--volume", dest="shared", action="append",help="Bind mount a volume, can work with multiple volumes" +
                        " format: HOST:CONTAINER:[rw|ro]")
    single_option = config_parser.add_mutually_exclusive_group()
    single_option.add_argument("-e", "--editor", dest="editor", default="vi", help="Editor to use")
    single_option.add_argument('--reset', action='store_true', help='Reset the configuration file to default')
    single_option.add_argument("--restore", action="store_true", help="Restore the previous version of PL/Container configuration")
    single_option.add_argument("--cleanup", action="store_true", help="Remove all the backup files for PL/Container")
    single_option.add_argument('-s', '--show', action='store_true', help='Show the contents of configuration file without changing it')

    # for install
    install_parser.add_argument("-i", "--image", dest="image", help="Configure container image location, support Tar.gz image" +
                        " file or docker image url")
    install_parser.add_argument("-n", "--name", dest="name", help="Configure container image name")
    install_parser.add_argument("-v", "--volume", dest="shared", action="append",help="Bind mount a volume, can work with multiple volumes" +
                        " format: HOST:CONTAINER:[rw|ro]")                    

    options = parser.parse_args()
    if options.subparser_name == "install":
        if not (options.name or options.image or options.shared):
            logger.error('unkown options')
            sys.exit(1)
        
        if not (options.name and options.image):
            logger.error('container namd and image must provied')
            sys.exit(1)

    elif options.subparser_name == "configure":
            if options.name and not options.image:
                logger.error('container namd and image must provied')
                sys.exit(1)

    return options

def distribut_docker_file(pool, d_file, dest, hosts):
    if dest is None:
        dest = os.path.join(TMP, d_file)
    for host in hosts:
        cmd = Scp('Distribute docker image file.',
                    d_file,
                    dest,
                    None,
                    host.strip())
        pool.addCommand(cmd)
    pool.join()
    pool.check_results()

def remote_cmd(pool, cmd_str, hosts):
    for host in hosts:
        cmd = Command('Execute remote cmd on all hosts.',
                        cmd_str,
                        ctxt = REMOTE,
                        remoteHost = host.strip())
        pool.addCommand(cmd)
    pool.join()
    results = pool.getCompletedItems()[0].get_results().stdout
    pool.check_results()
    return results

def get_docker_image(url):
    '''
    download docker image from given url
    '''
    logger.info("Getting docker image from %s" % url)
    (ok, out) = commands.getstatusoutput('docker pull %s' % url)
    if ok != 0:
        logger.error("Cannot pull docker image from %s" % url)
        sys.exit(1)
    name_tag = url.split("/")[-1]
    image_name = "".join([name_tag.replace(":", "_"), TAR_GZ])
    image_path = os.path.join(TMP, image_name)
    logger.info("Saving docker image to %s" % image_path)
    (ok, out) = commands.getstatusoutput('docker save -o %s %s' % (image_path, url))
    if ok != 0:
        logger.error("Cannot save docker image %s to %s" % (url, image_path))
        sys.exit(1)

    return image_path

def install_images(options, pool, hosts):
    '''
    install docker image either form url or file
    '''
    logger.info('Starting to install images %s' % (options.image))
    docker_image = options.image

    if docker_image is None:
        logger.error('Docker image cannot be none.')
        sys.exit(1)
    if not docker_image.endswith(TAR_GZ):
        docker_image = get_docker_image(options.image)

    try:
        logger.info('Checking docker installed on all hosts...')
        check_docker_cmd = 'which docker'
        remote_cmd(pool, check_docker_cmd, hosts)
        
        logger.info('Distributing image file %s to all hosts...' % docker_image)
        dest = os.path.join(GPHOME, PLCONTAINER_GPDB_DIR, docker_image)
        distribut_docker_file(pool, docker_image, dest, hosts)

        logger.info('Loading image on all hosts...')
        docker_load_cmd = " ".join(["docker load -i ", dest])
        dockerUrl = remote_cmd(pool, docker_load_cmd, hosts)
        # the returned format is "Loaded image: image:tag"
        return dockerUrl.split(":", 1)[1]

    except Exception, ex:
        raise(ex)

def update_xml_check_shared_folder(container, elements):
    '''
    update shared folder settings
    '''
    for shareDir in container.findall('shared_directory'):
        container.remove(shareDir)
    count = 0
    # default shared dir
    dest = os.path.join(GPHOME, PLCONTAINER_PYCLIENT_DIR)
    shareDir = ElementTree.Element("shared_directory",
            host=dest, container="/clientdir",
            access="ro")
    container.append(shareDir)

    if 'hasShared' in elements.keys():
        for plHost in elements['host']:
            shareDir = ElementTree.Element("shared_directory",
                    host=plHost, container=elements['container'][count],
                    access=elements['access'][count])
            count = count + 1
            container.append(shareDir)

def update_xml_container(container, elements):
    '''
    update name and image by given a container node
    '''
    isNew = True
    name = container.find('name').text
    if name == elements['name']:
        if userinput.ask_yesno(None, "This tag is already existed, do you want to overwrite?", 'N'):
            isNew = False
            image = container.find('image')
            image.text = elements['image']
            update_xml_check_shared_folder(container, elements)
        else:
            raise Exception("User interrupted")
    return isNew

def update_xml_new_container(elements):
    '''
    Create a new conatiner node
    '''
    container = ElementTree.Element("container")
    name = ElementTree.Element("name")
    name.text = elements['name']
    image = ElementTree.Element("image")
    image.text = elements['image']

    # default command
    command = ElementTree.Element("command")
    command.text = "/clientdir/pyclient"

    container.append(name)
    container.append(image)
    container.append(command)

    update_xml_check_shared_folder(container, elements)

    return container

def update_reformat_xml(xmltree):
    '''
    reformat xml to a pretty print format
    '''
    xmlString = ElementTree.tostring(xmltree)
    reformatString = xml.dom.minidom.parseString(xmlString)
    return reformatString.toprettyxml(indent = '    ') 

def update_xml_config(filename, elements):
    '''
    Update the xml file by given elements dict, if the given tag is
    not existed, then create a new tag. Otherwise, update the current
    tag.
    '''
    isNew = True
    try:
        xmltree = ElementTree.parse(filename)
        logger.info("file name is %s" % (filename))
        root = xmltree.getroot()

        for container in root.findall('container'):
            isNew = update_xml_container(container, elements)
            if not isNew:
                break

        if isNew:
           container = update_xml_new_container(elements)
           root.append(container)

        newXMLTreeString = update_reformat_xml(root)
        newXMLTreeString = os.linesep.join([s for s in newXMLTreeString.splitlines() if not s.strip() == ''])
        xmlfile = open(filename, "w")
        xmlfile.write(newXMLTreeString)
        xmlfile.close()
    except Exception, ex:
        logger.error("Update XML file Error")
        logger.error(str(ex))
        raise Exception("Update XML file Error")

def get_file_md5(file):
    '''
    Generates an MD5 hash of a file.  The hash returned is an array
    of bytes and cannot be printed directly to the screen or log file
    without converting to hex.
    '''
    h = hashlib.md5()
    f = open(file, 'r')
    for l in f:
        h.update(l)
    f.close()
    return h.digest()


def copy_file_to_tmp(host, file):
    """
    Copies file to a local temp file and returns the temp file name.
    While mktemp() is not "safe", the possibilities of this utility
    being run multiple times on the same file and getting the same
    temp filename is so remote that this is acceptable.
    """
    tmp_file = tempfile.mktemp()
    if not os.path.exists(file):
        default_config = os.path.join(GPHOME, PLCONTAINER_GPDB_DIR, PLCONTAINER_CONFIG_FILE)
        cmd = Scp('copy conf file from GPDB folder', default_config, file, host)
        cmd.run()
    cmd = Scp('copy conf file', file, tmp_file, host)
    cmd.run()
    if not os.path.exists(tmp_file):
        logger.error("Error while copying configuration file. Check that PL/Container is initialized" +
                     " or use --reset to initialize with default configuration")
        tmp_file = None
    return tmp_file


def validate_xml(filename):
    correct = True
    with open(filename, 'rb') as file:
        xmlcontents = file.read()
    try:
        ElementTree.fromstring(xmlcontents)
    except Exception, ex:
        correct = False
        logger.error("XML file does not conform XML specification")
        logger.error(str(ex))
    return correct


def edit_file(filename, editor):
    """
    Starts up options.editor (default is vi) to edit the file.
    """
    edit = True
    while edit:
        distribute = True
        h1 = get_file_md5(filename)
        os.system('%s %s' % (editor, filename))
        h2 = get_file_md5(filename)
        if h1 == h2:
            logger.info("File was not changed, no configuration update is done")
            distribute = False
        validxml = validate_xml(filename)
        if validxml:
            edit = False
        else:
            # If the file is invalid we won't distribute it
            distribute = False
            edit = userinput.ask_yesno(None, "Continue editing the file?", 'N')
    return distribute


def distribute_file(pool, src_filename, dest_filename, gpdb_locations):
    """
    Distributes the edited file to all GPDB instances
    """
    for l in gpdb_locations:
        dest_file = os.path.join(l[1], dest_filename)
        logger.info("Distributing to %s:%s", l[0], dest_file)
        cmd = Scp('distribute changed file',
                  src_filename,
                  dest_file,
                  None,
                  l[0])
        pool.addCommand(cmd)
    pool.join()
    pool.check_results()


def backup_file(pool, gpdb_locations, file):
    """
    Backs up a file to .bak
    """
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    dest_filename = '.' + file + '.bak' + timestamp
    logger.info("Old configuration is saved as '%s'" % dest_filename)
    for l in gpdb_locations:
        src_file = os.path.join(l[1], file)
        dest_file = os.path.join(l[1], dest_filename)
        cmd = Scp('backup file',
                  src_file,
                  dest_file,
                  l[0],
                  l[0])
        pool.addCommand(cmd)
    pool.join()
    pool.check_results()

def get_all_host(array):
    hosts = set()
    for hostname in array.get_hostlist():
       hosts.add(hostname)
    return hosts

def get_segment_locations(seg_list):
    """
    Returns a list of tuples that are of the format (hostname, gpdb directory)
    for every segment instance in the cluster.
    """
    gpdb_locations = []
    for s in seg_list:
        gpdb_locations.append((s.getSegmentHostName(), s.getSegmentDataDirectory()))
    return gpdb_locations


def get_last_backup(pool, remote_host, remote_dir, filename):
    """
    Get the last backup file name of PL/Container configuration
    """
    result = None
    filemask = os.path.join(remote_dir, '.' + filename + '.bak*')
    cmdstr = 'ls -1 %s || exit 0' % filemask
    cmd = Command('List PL/Contaner configuartion backups in %s:%s' % (remote_host, remote_dir),
                  cmdstr,
                  ctxt = REMOTE,
                  remoteHost = remote_host)
    cmd.run(validateAfter=True)
    logger.debug("Command executed: '%s'" % cmdstr)
    res = cmd.get_results()
    if res.rc != 0:
        logger.error("Cannot find configuration backup files")
    else:
        if res.stdout.strip() != '':
            files = [x.strip() for x in res.stdout.strip().split('\n')]
            logger.info("Found %d configuration backups" % len(files))
            files = sorted(files)
            for f in files:
                logger.debug("    %s" % f)
            result = files[-1]
        else:
            logger.error("No configuration backups found")
    return result


def remove_files(pool, gpdb_locations, remote_filename):
    """
    Removes the specified file from all the hosts of GPDB cluster
    """
    for l in gpdb_locations:
        remote_file = os.path.join(l[1], remote_filename)
        cmd = Command('Remove %s:%s' % (l[0], remote_file),
                      'rm %s || exit 0' % remote_file,
                      ctxt = REMOTE,
                      remoteHost = l[0])
        pool.addCommand(cmd)
    pool.join()
    pool.check_results()


def show_file(filename):
    logger.info("Configuration file contents:")
    f = open(filename, 'rb')
    sys.stdout.write('%s\n' % f.read())
    f.close()

def get_conf_shared_dir(options, elements):
    '''
    get the shared dirs
    '''
    elements['hasShared'] = True
    shareStrs = options.shared
    elements['host'] = list()
    elements['container'] = list()
    elements['access'] = list()
    for shareStr in shareStrs:
        strList = shareStr.split(":")
        elements['host'].append(strList[0])
        elements['container'].append(strList[1])
        if len(strList) >= 3:
            if strList[2] == "ro" or strList[2] == "rw":
                elements['access'].append(strList[2])
            else:
                raise Exception("Access type can only be \"ro\" or \"rw\". ")
        else:
            elements['access'].append("ro")


def get_conf_elements(options, dockImage = None):
    '''
    build an element dict for configuration
    '''
    elements = dict()
    elements['name'] = options.name
    if dockImage:
        elements['image'] = dockImage.strip()
    else: 
        elements['image'] = options.image
    if options.shared:
       get_conf_shared_dir(options, elements)
    return elements

def main():
    pool = None
    tmp_file = None
    options = None
    new_config = False

    global GPHOME

    options = parseargs()
    if options.verbose:
        gplog.enable_verbose_logging()

    try:
        GPHOME = os.environ.get('GPHOME')
        if not GPHOME:
            raise Exception('$GPHOME environment variable is not set')

        logger.info('Retrieving GPDB configuration from database...')
        url = dbconn.DbURL()
        array = gparray.GpArray.initFromCatalog(url, True)
        segs = array.getDbList()
        hosts = get_all_host(array)

        gpdb_locations = get_segment_locations(segs)
        logger.debug('GPDB Configuration is:')

        remote_dir = gpdb_locations[0][1]
        remote_host = gpdb_locations[0][0]
        remote_filename = os.path.join(remote_dir, PLCONTAINER_CONFIG_FILE)
        tmp_file = copy_file_to_tmp(remote_host, remote_filename)

        for host, directory in gpdb_locations:
            logger.debug('    host "%s" directory "%s"' % (host, directory))

        if options.subparser_name == "configure" and options.show:
            logger.info('Creating temporary copy of %s from %s...' % (remote_filename, remote_host))
            show_file(tmp_file)

        if options.subparser_name == "configure" and options.reset:
            logger.info('Preparing the worker pool...')
            pool = WorkerPool(PL_BATCH_SIZE)
            default_config = os.path.join(GPHOME, PLCONTAINER_GPDB_DIR, PLCONTAINER_CONFIG_FILE)
            logger.info('Distributing file %s to all locations...' % default_config)
            distribute_file(pool, default_config, PLCONTAINER_CONFIG_FILE, gpdb_locations)
            new_config = True

        if options.subparser_name == "configure" and options.restore:
            logger.info('Getting list of configuration backups in %s:%s...' % (remote_host, remote_dir))
            filename = get_last_backup(pool, remote_host, remote_dir, PLCONTAINER_CONFIG_FILE)
            if filename and userinput.ask_yesno(None, "Continue restoring the backup configuration '%s'?" % filename, 'N'):
                pool = WorkerPool(PL_BATCH_SIZE)
                logger.info('Distributing file %s to all locations...' % PLCONTAINER_CONFIG_FILE)
                distribute_file(pool, tmp_file, PLCONTAINER_CONFIG_FILE, gpdb_locations)
                logger.info('Cleaning up old backup file...')
                remove_files(pool, gpdb_locations, os.path.basename(filename))
                new_config = True

        if options.name and options.subparser_name == "configure":
            elements = get_conf_elements(options)
            update_xml_config(tmp_file, elements)
            logger.info('Preparing the worker pool...')
            pool = WorkerPool(PL_BATCH_SIZE)
            logger.info('Backing up file %s on all hosts...' % PLCONTAINER_CONFIG_FILE)
            backup_file(pool, gpdb_locations, PLCONTAINER_CONFIG_FILE)
            logger.info('Distributing file %s to all locations...' % PLCONTAINER_CONFIG_FILE)
            distribute_file(pool, tmp_file, PLCONTAINER_CONFIG_FILE, gpdb_locations)
            new_config = True
        
        if options.name and options.subparser_name == "install":
            logger.info('Preparing the worker pool...')
            pool = WorkerPool(PL_BATCH_SIZE)
            dockerImageName = install_images(options, pool, hosts)
            elements = get_conf_elements(options, dockerImageName)
            update_xml_config(tmp_file, elements)
            logger.info('Distributing file %s to all locations...' % PLCONTAINER_CONFIG_FILE)
            distribute_file(pool, tmp_file, PLCONTAINER_CONFIG_FILE, gpdb_locations)
            new_config = True


        if options.subparser_name == "configure" and options.restore:
            if userinput.ask_yesno(None, "Are you sure want to clean up the PL/Container backup configurations?", 'N'):
                logger.info('Preparing the worker pool...')
                pool = WorkerPool(PL_BATCH_SIZE)
                logger.info('Cleaning up old backup files...')
                remove_files(pool, gpdb_locations, '.' + PLCONTAINER_CONFIG_FILE + '.bak*')

        if options.subparser_name == "configure" and options.editor and not options.name:
            changed = edit_file(tmp_file, options.editor)
            if changed:
                logger.info('Preparing the worker pool...')
                pool = WorkerPool(PL_BATCH_SIZE)
                logger.info('Backing up file %s on all hosts...' % PLCONTAINER_CONFIG_FILE)
                backup_file(pool, gpdb_locations, PLCONTAINER_CONFIG_FILE)
                logger.info('Distributing file %s to all locations...' % PLCONTAINER_CONFIG_FILE)
                distribute_file(pool, tmp_file, PLCONTAINER_CONFIG_FILE, gpdb_locations)
                new_config = True

        if new_config:
            logger.info('Configuration has changed. Run "select * from plcontainer_refresh_config" in open sessions.' +
                        ' New sessions will get new configuration automatically')

    except Exception, ex:
        logger.fatal("%s failed. (Reason='%s') exiting..." % (execname, ex))
        if options.verbose:
            logger.exception(e)

    finally:
        if tmp_file:
            os.unlink(tmp_file)
        if pool:
            logger.info('Shutting down workers...')
            pool.haltWork()
            pool.joinWorkers()
        logger.info('Done')


if __name__ == '__main__':
    execname = os.path.split(__file__)[-1]
    gplog.setup_tool_logging(execname, getLocalHostname(), getUserName())
    logger = gplog.get_default_logger()
    main()
